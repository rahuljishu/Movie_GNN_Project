{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c324d1e-3371-41fa-b0a2-9efe86bbe9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1\n",
      "PyG version: 2.6.1\n",
      "\n",
      "GPU is available! Using device: cuda\n",
      "Device name: NVIDIA GeForce GTX 1650\n",
      "\n",
      "Loading the graph from the pickle file...\n",
      "Loading MovieLens ratings data...\n",
      "\n",
      "--- Data Loaded Successfully ---\n",
      "Graph has 1843257 nodes and 3957929 edges.\n",
      "Loaded 33832162 user ratings.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1225734739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>110</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1225865086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1225733503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>260</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1225735204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>356</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1225735119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating   timestamp\n",
       "0       1        1     4.0  1225734739\n",
       "1       1      110     4.0  1225865086\n",
       "2       1      158     4.0  1225733503\n",
       "3       1      260     4.5  1225735204\n",
       "4       1      356     5.0  1225735119"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Step 1: Import Libraries and Verify GPU ---\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Import our new deep learning libraries\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"PyG version: {torch_geometric.__version__}\")\n",
    "\n",
    "# --- GPU VERIFICATION ---\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\\nGPU is available! Using device: {device}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\\nGPU not available, using CPU. Training will be slower.\")\n",
    "\n",
    "# Define our data paths\n",
    "PROCESSED_DATA_PATH = \"../data/processed/\"\n",
    "RAW_DATA_PATH = \"../data/raw/\"\n",
    "\n",
    "# Load the main bipartite graph we created in Notebook 1\n",
    "print(\"\\nLoading the graph from the pickle file...\")\n",
    "graph_path = os.path.join(PROCESSED_DATA_PATH, \"movie_actor_graph.gpickle\")\n",
    "with open(graph_path, 'rb') as f:\n",
    "    B = pickle.load(f)\n",
    "\n",
    "# Load the MovieLens ratings data, which will be our prediction target\n",
    "print(\"Loading MovieLens ratings data...\")\n",
    "ratings_df = pd.read_csv(os.path.join(RAW_DATA_PATH, \"ml-latest\", \"ratings.csv\"))\n",
    "\n",
    "print(\"\\n--- Data Loaded Successfully ---\")\n",
    "print(f\"Graph has {B.number_of_nodes()} nodes and {B.number_of_edges()} edges.\")\n",
    "print(f\"Loaded {len(ratings_df)} user ratings.\")\n",
    "\n",
    "display(ratings_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "234de203-1c1c-4a13-9f7a-af275dd053a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of ratings: 33832162\n",
      "Using a subset of 3377660 ratings from 33097 users for faster training.\n",
      "\n",
      "Final counts for GNN data structure:\n",
      " - Users: 33081\n",
      " - Movies: 33452\n",
      " - Actors: 1314337\n",
      " - Ratings (edges): 3304655\n"
     ]
    }
   ],
   "source": [
    "# --- Step 2 Sub-sample Ratings and Create Mappings ---\n",
    "\n",
    "# To make training faster, let's work with a subset of the ratings.\n",
    "# Let's start with 10% of the users.\n",
    "all_user_ids = ratings_df['userId'].unique()\n",
    "sample_user_ids = np.random.choice(all_user_ids, size=int(len(all_user_ids) * 0.10), replace=False)\n",
    "\n",
    "# Filter the ratings dataframe to only include these users\n",
    "ratings_subset_df = ratings_df[ratings_df['userId'].isin(sample_user_ids)].copy()\n",
    "\n",
    "print(f\"Original number of ratings: {len(ratings_df)}\")\n",
    "print(f\"Using a subset of {len(ratings_subset_df)} ratings from {len(sample_user_ids)} users for faster training.\")\n",
    "\n",
    "# --- Create Mappings ---\n",
    "# We need a continuous integer index for each node type (0, 1, 2, ...)\n",
    "\n",
    "# The 'links.csv' file is the key to mapping between MovieLens movieId and IMDb tconst\n",
    "links_df = pd.read_csv(os.path.join(RAW_DATA_PATH, \"ml-latest\", \"links.csv\"), dtype={'imdbId': str})\n",
    "# Prepend 'tt' to imdbId to match the format in our graph B (e.g., 114709 -> tt0114709)\n",
    "links_df['imdbId'] = 'tt' + links_df['imdbId'].str.zfill(7)\n",
    "# Create a dictionary to map from MovieLens ID -> IMDb ID\n",
    "ml_to_imdb_map = links_df.set_index('movieId')['imdbId'].to_dict()\n",
    "\n",
    "\n",
    "# 1. Identify valid movies present in BOTH our graph AND the ratings subset\n",
    "graph_movie_ids = {node for node, data in B.nodes(data=True) if data.get('type') == 'movie'}\n",
    "rated_movie_ids_ml = set(ratings_subset_df['movieId'].unique())\n",
    "\n",
    "valid_imdb_ids_from_graph = set(ml_to_imdb_map.values()).intersection(graph_movie_ids)\n",
    "valid_ml_ids = {ml_id for ml_id, imdb_id in ml_to_imdb_map.items() if imdb_id in valid_imdb_ids_from_graph}\n",
    "final_valid_ml_ids = valid_ml_ids.intersection(rated_movie_ids_ml)\n",
    "\n",
    "\n",
    "# 2. Filter our ratings dataframe one last time to ensure all movies/users are valid\n",
    "ratings_final_df = ratings_subset_df[ratings_subset_df['movieId'].isin(final_valid_ml_ids)].copy()\n",
    "final_valid_user_ids = ratings_final_df['userId'].unique()\n",
    "\n",
    "\n",
    "# 3. Create the final mappings\n",
    "user_mapping = {user_id: i for i, user_id in enumerate(final_valid_user_ids)}\n",
    "movie_mapping = {movie_id: i for i, movie_id in enumerate(final_valid_ml_ids)}\n",
    "# A robust way to get actor nodes\n",
    "actor_nodes = {node for node, data in B.nodes(data=True) if data.get('type') == 'actor'}\n",
    "actor_mapping = {actor_id: i for i, actor_id in enumerate(actor_nodes)}\n",
    "\n",
    "\n",
    "print(f\"\\nFinal counts for GNN data structure:\")\n",
    "print(f\" - Users: {len(user_mapping)}\")\n",
    "print(f\" - Movies: {len(movie_mapping)}\")\n",
    "print(f\" - Actors: {len(actor_mapping)}\")\n",
    "print(f\" - Ratings (edges): {len(ratings_final_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5d5282b-929d-45a3-afc9-9a928af01d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created HeteroData object with user, movie, and actor nodes.\n",
      "Adding User-Movie edges...\n",
      "Adding Actor-Movie edges...\n",
      "\n",
      "Moving data to the GPU...\n",
      "\n",
      "Added edges and moved data to the GPU successfully:\n",
      "HeteroData(\n",
      "  user={ num_nodes=33081 },\n",
      "  movie={ num_nodes=33452 },\n",
      "  actor={ num_nodes=1314337 },\n",
      "  (user, rates, movie)={\n",
      "    edge_index=[2, 3304655],\n",
      "    edge_attr=[3304655],\n",
      "  },\n",
      "  (actor, acted_in, movie)={ edge_index=[2, 323257] },\n",
      "  (movie, has_actor, actor)={ edge_index=[2, 323257] }\n",
      ")\n",
      "\n",
      "Is the data on the GPU? True\n"
     ]
    }
   ],
   "source": [
    "# --- Step 3  Create the HeteroData Object ---\n",
    "\n",
    "data = HeteroData()\n",
    "\n",
    "# Add the nodes.\n",
    "data['user'].num_nodes = len(user_mapping)\n",
    "data['movie'].num_nodes = len(movie_mapping)\n",
    "data['actor'].num_nodes = len(actor_mapping)\n",
    "\n",
    "print(\"Created HeteroData object with user, movie, and actor nodes.\")\n",
    "\n",
    "# --- Define the Edges ---\n",
    "\n",
    "# 1. User-Movie Edges (from ratings)\n",
    "print(\"Adding User-Movie edges...\")\n",
    "user_indices = [user_mapping[uid] for uid in ratings_final_df['userId']]\n",
    "movie_indices_for_rating = [movie_mapping[mid] for mid in ratings_final_df['movieId']]\n",
    "\n",
    "data['user', 'rates', 'movie'].edge_index = torch.tensor([user_indices, movie_indices_for_rating])\n",
    "data['user', 'rates', 'movie'].edge_attr = torch.tensor(ratings_final_df['rating'].values, dtype=torch.float)\n",
    "\n",
    "\n",
    "# 2. Actor-Movie Edges (from our graph B)\n",
    "print(\"Adding Actor-Movie edges...\")\n",
    "imdb_to_ml_map = {v: k for k, v in ml_to_imdb_map.items()}\n",
    "\n",
    "actor_edge_indices = []\n",
    "movie_edge_indices = []\n",
    "\n",
    "# --- THIS IS THE FIX ---\n",
    "# nx.bipartite.edges is deprecated. We iterate through all edges and check node types.\n",
    "# This is the modern, robust way to do it.\n",
    "for u, v in B.edges():\n",
    "    # Check if u is a movie and v is an actor\n",
    "    if B.nodes[u].get('type') == 'movie' and B.nodes[v].get('type') == 'actor':\n",
    "        movie_id_str, actor_id_str = u, v\n",
    "    # Check if v is a movie and u is an actor\n",
    "    elif B.nodes[v].get('type') == 'movie' and B.nodes[u].get('type') == 'actor':\n",
    "        movie_id_str, actor_id_str = v, u\n",
    "    else:\n",
    "        continue # Skip if it's not a movie-actor edge\n",
    "\n",
    "    # Now, proceed with the same logic as before\n",
    "    if movie_id_str in imdb_to_ml_map:\n",
    "        ml_movie_id = imdb_to_ml_map[movie_id_str]\n",
    "        if ml_movie_id in movie_mapping:\n",
    "            movie_idx = movie_mapping[ml_movie_id]\n",
    "            if actor_id_str in actor_mapping:\n",
    "                actor_idx = actor_mapping[actor_id_str]\n",
    "                actor_edge_indices.append(actor_idx)\n",
    "                movie_edge_indices.append(movie_idx)\n",
    "\n",
    "# Create the final edge tensors for the graph structure\n",
    "data['actor', 'acted_in', 'movie'].edge_index = torch.tensor([actor_edge_indices, movie_edge_indices], dtype=torch.long)\n",
    "data['movie', 'has_actor', 'actor'].edge_index = torch.tensor([movie_edge_indices, actor_edge_indices], dtype=torch.long)\n",
    "\n",
    "# --- Final Step: Send the data to the GPU ---\n",
    "print(\"\\nMoving data to the GPU...\")\n",
    "data = data.to(device)\n",
    "\n",
    "print(\"\\nAdded edges and moved data to the GPU successfully:\")\n",
    "print(data)\n",
    "print(f\"\\nIs the data on the GPU? {data.is_cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81f2c86f-93fd-48f5-a19f-977b891c2707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Corrected HeteroGNN Model architecture defined successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 4 Define the Explicit HeteroGNN Model ---\n",
    "\n",
    "from torch_geometric.nn import SAGEConv, HeteroConv\n",
    "\n",
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = HeteroConv({\n",
    "            ('user', 'rates', 'movie'): SAGEConv((-1, -1), hidden_channels),\n",
    "            ('actor', 'acted_in', 'movie'): SAGEConv((-1, -1), hidden_channels),\n",
    "            ('movie', 'has_actor', 'actor'): SAGEConv((-1, -1), hidden_channels),\n",
    "            ('movie', 'rev_rates', 'user'): SAGEConv((-1, -1), hidden_channels),\n",
    "        }, aggr='sum')\n",
    "        self.conv2 = HeteroConv({\n",
    "            ('user', 'rates', 'movie'): SAGEConv((-1, -1), hidden_channels),\n",
    "            ('actor', 'acted_in', 'movie'): SAGEConv((-1, -1), hidden_channels),\n",
    "            ('movie', 'has_actor', 'actor'): SAGEConv((-1, -1), hidden_channels),\n",
    "            ('movie', 'rev_rates', 'user'): SAGEConv((-1, -1), hidden_channels),\n",
    "        }, aggr='sum')\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        x_dict = self.conv1(x_dict, edge_index_dict)\n",
    "        x_dict = {key: x.relu() for key, x in x_dict.items()}\n",
    "        x_dict = self.conv2(x_dict, edge_index_dict)\n",
    "        return x_dict\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.user_emb = torch.nn.Embedding(data['user'].num_nodes, hidden_channels)\n",
    "        self.movie_emb = torch.nn.Embedding(data['movie'].num_nodes, hidden_channels)\n",
    "        self.actor_emb = torch.nn.Embedding(data['actor'].num_nodes, hidden_channels)\n",
    "        self.gnn = HeteroGNN(hidden_channels)\n",
    "        self.decoder = lambda x_user, x_movie: (x_user * x_movie).sum(dim=-1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # 1. Get initial embeddings\n",
    "        x_dict = {\n",
    "          \"user\": self.user_emb(data[\"user\"].node_id),\n",
    "          \"movie\": self.movie_emb(data[\"movie\"].node_id),\n",
    "          \"actor\": self.actor_emb(data[\"actor\"].node_id),\n",
    "        } \n",
    "        \n",
    "        # 2. Run GNN encoder to get final embeddings for ALL nodes\n",
    "        x_dict = self.gnn(x_dict, data.edge_index_dict)\n",
    "        \n",
    "        # --- THIS IS THE FIX ---\n",
    "        # 3. Decode predictions ONLY for the edges we are interested in\n",
    "        #    The splitter puts the target edges in 'edge_label_index'\n",
    "        edge_label_index = data['user', 'rates', 'movie'].edge_label_index\n",
    "        \n",
    "        pred = self.decoder(\n",
    "            x_dict['user'][edge_label_index[0]],\n",
    "            x_dict['movie'][edge_label_index[1]],\n",
    "        )\n",
    "        \n",
    "        return pred\n",
    "\n",
    "print(\"Final Corrected HeteroGNN Model architecture defined successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8db44328-185e-4c8f-bce9-ecedf6483154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial node IDs created and assigned.\n",
      "HeteroData(\n",
      "  user={\n",
      "    num_nodes=33081,\n",
      "    node_id=[33081],\n",
      "  },\n",
      "  movie={\n",
      "    num_nodes=33452,\n",
      "    node_id=[33452],\n",
      "  },\n",
      "  actor={\n",
      "    num_nodes=1314337,\n",
      "    node_id=[1314337],\n",
      "  },\n",
      "  (user, rates, movie)={\n",
      "    edge_index=[2, 3304655],\n",
      "    edge_attr=[3304655],\n",
      "  },\n",
      "  (actor, acted_in, movie)={ edge_index=[2, 323257] },\n",
      "  (movie, has_actor, actor)={ edge_index=[2, 323257] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# --- Step 5 Create Initial Node Features ---\n",
    "\n",
    "# All our model needs are the unique IDs for each node, which the\n",
    "# Embedding layers will use to look up the feature vectors.\n",
    "\n",
    "data['user'].node_id = torch.arange(data['user'].num_nodes, device=device)\n",
    "data['movie'].node_id = torch.arange(data['movie'].num_nodes, device=device)\n",
    "data['actor'].node_id = torch.arange(data['actor'].num_nodes, device=device)\n",
    "\n",
    "# We no longer need the '.x' attribute for movies. Let's delete it to be clean.\n",
    "del data['movie'].x\n",
    "\n",
    "print(\"Initial node IDs created and assigned.\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d694c1e-15e3-45ae-9346-f0e2a6703742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Splitting Complete ---\n",
      "\n",
      "Training Data:\n",
      "HeteroData(\n",
      "  user={\n",
      "    num_nodes=33081,\n",
      "    node_id=[33081],\n",
      "  },\n",
      "  movie={\n",
      "    num_nodes=33452,\n",
      "    node_id=[33452],\n",
      "  },\n",
      "  actor={\n",
      "    num_nodes=1314337,\n",
      "    node_id=[1314337],\n",
      "  },\n",
      "  (user, rates, movie)={\n",
      "    edge_index=[2, 2643725],\n",
      "    edge_attr=[2643725],\n",
      "    edge_label=[2643725],\n",
      "    edge_label_index=[2, 2643725],\n",
      "  },\n",
      "  (actor, acted_in, movie)={ edge_index=[2, 323257] },\n",
      "  (movie, has_actor, actor)={ edge_index=[2, 323257] },\n",
      "  (movie, rev_rates, user)={\n",
      "    edge_index=[2, 2643725],\n",
      "    edge_attr=[2643725],\n",
      "    edge_label=[2643725],\n",
      "  },\n",
      "  (movie, rev_acted_in, actor)={ edge_index=[2, 323257] },\n",
      "  (actor, rev_has_actor, movie)={ edge_index=[2, 323257] },\n",
      "  (user, rev_rev_rates, movie)={\n",
      "    edge_index=[2, 2643725],\n",
      "    edge_attr=[2643725],\n",
      "    edge_label=[2643725],\n",
      "  }\n",
      ")\n",
      "\n",
      "Validation Data:\n",
      "HeteroData(\n",
      "  user={\n",
      "    num_nodes=33081,\n",
      "    node_id=[33081],\n",
      "  },\n",
      "  movie={\n",
      "    num_nodes=33452,\n",
      "    node_id=[33452],\n",
      "  },\n",
      "  actor={\n",
      "    num_nodes=1314337,\n",
      "    node_id=[1314337],\n",
      "  },\n",
      "  (user, rates, movie)={\n",
      "    edge_index=[2, 2643725],\n",
      "    edge_attr=[2643725],\n",
      "    edge_label=[660930],\n",
      "    edge_label_index=[2, 660930],\n",
      "  },\n",
      "  (actor, acted_in, movie)={ edge_index=[2, 323257] },\n",
      "  (movie, has_actor, actor)={ edge_index=[2, 323257] },\n",
      "  (movie, rev_rates, user)={\n",
      "    edge_index=[2, 2643725],\n",
      "    edge_attr=[2643725],\n",
      "  },\n",
      "  (movie, rev_acted_in, actor)={ edge_index=[2, 323257] },\n",
      "  (actor, rev_has_actor, movie)={ edge_index=[2, 323257] },\n",
      "  (user, rev_rev_rates, movie)={\n",
      "    edge_index=[2, 2643725],\n",
      "    edge_attr=[2643725],\n",
      "  }\n",
      ")\n",
      "\n",
      "Test Data:\n",
      "HeteroData(\n",
      "  user={\n",
      "    num_nodes=33081,\n",
      "    node_id=[33081],\n",
      "  },\n",
      "  movie={\n",
      "    num_nodes=33452,\n",
      "    node_id=[33452],\n",
      "  },\n",
      "  actor={\n",
      "    num_nodes=1314337,\n",
      "    node_id=[1314337],\n",
      "  },\n",
      "  (user, rates, movie)={\n",
      "    edge_index=[2, 2974190],\n",
      "    edge_attr=[2974190],\n",
      "    edge_label=[660930],\n",
      "    edge_label_index=[2, 660930],\n",
      "  },\n",
      "  (actor, acted_in, movie)={ edge_index=[2, 323257] },\n",
      "  (movie, has_actor, actor)={ edge_index=[2, 323257] },\n",
      "  (movie, rev_rates, user)={\n",
      "    edge_index=[2, 2974190],\n",
      "    edge_attr=[2974190],\n",
      "  },\n",
      "  (movie, rev_acted_in, actor)={ edge_index=[2, 323257] },\n",
      "  (actor, rev_has_actor, movie)={ edge_index=[2, 323257] },\n",
      "  (user, rev_rev_rates, movie)={\n",
      "    edge_index=[2, 2974190],\n",
      "    edge_attr=[2974190],\n",
      "  }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# --- Step 6 Split Edges for Training, Validation, and Testing ---\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "# We will split the 'user' -> 'rates' -> 'movie' edges.\n",
    "# By setting is_undirected=False and providing the reverse edge type,\n",
    "# we ensure the transform creates the necessary reverse edges for GNN message passing.\n",
    "transform = T.RandomLinkSplit(\n",
    "    num_val=0.1,\n",
    "    num_test=0.1,\n",
    "    is_undirected=False, # We will handle the reverse edges explicitly\n",
    "    add_negative_train_samples=False,\n",
    "    edge_types=[('user', 'rates', 'movie')],\n",
    "    rev_edge_types=[('movie', 'rev_rates', 'user')] # Tell the splitter to create these reverse edges\n",
    ")\n",
    "\n",
    "# Apply the transform to our data\n",
    "train_data, val_data, test_data = transform(data)\n",
    "\n",
    "\n",
    "# --- CRUCIAL FIX: Make the actor-movie connections undirected manually ---\n",
    "# The splitter only works on the edge type we give it. We need to ensure\n",
    "# the other structural edges are also treated as undirected for message passing.\n",
    "train_data = ToUndirected()(train_data)\n",
    "val_data = ToUndirected()(val_data)\n",
    "test_data = ToUndirected()(test_data)\n",
    "\n",
    "\n",
    "print(\"--- Data Splitting Complete ---\")\n",
    "print(\"\\nTraining Data:\")\n",
    "print(train_data)\n",
    "print(\"\\nValidation Data:\")\n",
    "print(val_data)\n",
    "print(\"\\nTest Data:\")\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ccf004e4-1c5b-4eae-b7f9-0157262e9717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the full training run for 50 epochs...\n",
      "Model will be saved to the absolute path: C:\\Users\\rahul\\OneDrive\\Documents\\Movie_GNN_Project\\models\\gnn_recommendation_model.pt\n",
      "Epoch 05, Train Loss: 2.4819, Validation RMSE: 2.4650\n",
      "Epoch 10, Train Loss: 10.1088, Validation RMSE: 2.1785\n",
      "Epoch 15, Train Loss: 2.1751, Validation RMSE: 2.1038\n",
      "Epoch 20, Train Loss: 0.1227, Validation RMSE: 0.8993\n",
      "Epoch 25, Train Loss: 0.3314, Validation RMSE: 0.7854\n",
      "Epoch 30, Train Loss: 0.3916, Validation RMSE: 0.7802\n",
      "Epoch 35, Train Loss: 0.1891, Validation RMSE: 0.7877\n",
      "Epoch 40, Train Loss: 0.0471, Validation RMSE: 0.9600\n",
      "Epoch 45, Train Loss: 0.0774, Validation RMSE: 1.1250\n",
      "Epoch 50, Train Loss: 0.0664, Validation RMSE: 1.0678\n",
      "\n",
      "--- Full Training Complete ---\n",
      "\n",
      "Trained model state saved to: ../models/gnn_recommendation_model.pt\n",
      "\n",
      "SUCCESS: File has been verified and exists in the correct 'models' folder!\n"
     ]
    }
   ],
   "source": [
    "# --- Step 7 (Final & Robust): Initialize and Train the Model (Full Run) ---\n",
    "from tqdm.notebook import tqdm \n",
    "\n",
    "# --- Model Initialization ---\n",
    "hidden_channels = 64 \n",
    "model = Model(hidden_channels=hidden_channels).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_function = torch.nn.MSELoss()\n",
    "\n",
    "\n",
    "# --- Training Loop ---\n",
    "# We are running the full 50 epochs for the best performance.\n",
    "epochs = 50 \n",
    "\n",
    "# --- Explicit Save Path ---\n",
    "# We are using an explicit relative path to go UP from the 'notebooks' folder\n",
    "# and then DOWN into the correct 'models' folder.\n",
    "MODELS_DIR = \"../models/\"\n",
    "os.makedirs(MODELS_DIR, exist_ok=True) # Create the directory if it's not there\n",
    "model_save_path = os.path.join(MODELS_DIR, \"gnn_recommendation_model.pt\")\n",
    "\n",
    "\n",
    "print(f\"Starting the full training run for {epochs} epochs...\")\n",
    "print(f\"Model will be saved to the absolute path: {os.path.abspath(model_save_path)}\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(train_data)\n",
    "    ground_truth = train_data['user', 'rates', 'movie'].edge_label\n",
    "    loss = loss_function(pred, ground_truth)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_pred = model(val_data)\n",
    "        val_ground_truth = val_data['user', 'rates', 'movie'].edge_label\n",
    "        val_loss = loss_function(val_pred, val_ground_truth)\n",
    "        \n",
    "    val_rmse = torch.sqrt(val_loss)\n",
    "\n",
    "    # Print progress every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1:02d}, Train Loss: {loss.item():.4f}, Validation RMSE: {val_rmse.item():.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Full Training Complete ---\")\n",
    "\n",
    "# --- Save the Trained Model to the Correct Location ---\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"\\nTrained model state saved to: {model_save_path}\")\n",
    "\n",
    "# Final verification to be 100% sure the file exists\n",
    "if os.path.exists(model_save_path):\n",
    "    print(\"\\nSUCCESS: File has been verified and exists in the correct 'models' folder!\")\n",
    "else:\n",
    "    print(\"\\nCRITICAL ERROR: File still not found after saving. Please check folder permissions or disk space.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab7b4a1-3d43-4013-845b-fde0f9ee5a8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
